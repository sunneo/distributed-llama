#Plan to Mix distributed-llm with air-llm

1. align distributed-llama to python
2. clone a airllm
3. make every model distributed to all nodes. (Shared-Storage Zero-Data Movement)
4. mix airllm + distributed-llama
5. under this target, re-organize / rewrite bottleneck to c/c++
