# Distributed-AirLLM: Quick Reference

> **For**: Users who want a quick overview  
> **Version**: 1.0  
> **Last Updated**: 2026-01-25

## üéØ What is Distributed-AirLLM?

A hybrid system combining:
- **Distributed-Llama**: Distributed tensor parallelism
- **AirLLM**: Layer-wise memory management

**Key Innovation**: Shared-storage architecture where each node has the full model on disk but loads only assigned layers to RAM.

## üìä Quick Stats

- **Status**: 82% complete (4 phases implemented)
- **Network Reduction**: 73.4% (activation compression)
- **Memory Efficiency**: 50% less RAM than traditional distributed
- **Performance**: 5-15x speedup with C++ extensions
- **Deployment**: Multi-node CPU clusters

## üöÄ Quick Start

### Prerequisites
- Multiple machines with 8-16GB RAM each
- 1 Gbps+ network
- Same model file on all nodes

### 3-Step Deployment

```bash
# Step 1: On all worker nodes
cd mix/target/distributed-llama.python
pip install -r requirements.txt
python -m worker --host 0.0.0.0 --port 9999 --model /path/to/model.m

# Step 2: On root node
cd /home/runner/work/distributed-llama/distributed-llama
make dllama

# Step 3: Start inference
./dllama chat \
    --model /path/to/model.m \
    --tokenizer /path/to/tokenizer.t \
    --workers 192.168.1.2:9999 192.168.1.3:9999
```

## üìö Documentation Index

### For Deployment
‚Üí **[DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)**
- Step-by-step setup instructions
- Multiple deployment scenarios
- Troubleshooting guide
- Performance tuning

### For Understanding
‚Üí **[COMPARISON_AND_ADVANTAGES.md](COMPARISON_AND_ADVANTAGES.md)**
- How it compares to AirLLM
- How it compares to Distributed-Llama
- Technical advantages
- Use case recommendations

### For Developers
‚Üí **[TODO_AND_IMPROVEMENTS.md](TODO_AND_IMPROVEMENTS.md)**
- Complete TODO list
- Improvement suggestions
- Priority breakdown
- Contribution guide

### For Technical Details
‚Üí **[IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)**
- Implementation details
- Architecture overview
- Code structure

‚Üí **[PHASE3_4_SUMMARY.md](PHASE3_4_SUMMARY.md)**
- Network optimizations
- C++ extensions
- Performance benchmarks

## üéØ Common Questions

### Q: Where do I start if I want to deploy?
**A:** Follow [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) ‚Üí "Quick Start" section

### Q: How is this different from AirLLM?
**A:** See [COMPARISON_AND_ADVANTAGES.md](COMPARISON_AND_ADVANTAGES.md) ‚Üí "vs. AirLLM" section
- AirLLM: Single machine, slow but memory-efficient
- Distributed-AirLLM: Multi-machine, fast AND memory-efficient

### Q: How is this different from Distributed-Llama?
**A:** See [COMPARISON_AND_ADVANTAGES.md](COMPARISON_AND_ADVANTAGES.md) ‚Üí "vs. Distributed-Llama" section
- Distributed-Llama: Full shards in RAM, high memory
- Distributed-AirLLM: Layer-wise loading, 50% less RAM

### Q: What are the advantages?
**A:** Main advantages:
1. **50% less RAM** than traditional distributed (layer-wise loading)
2. **73% less network traffic** (activation compression)
3. **5-15x faster** computation (SIMD optimization)
4. **Better fault tolerance** (shared storage, any node can load any layer)
5. **Cheaper hardware** (consumer-grade CPUs, no GPU required)

### Q: What still needs to be done?
**A:** See [TODO_AND_IMPROVEMENTS.md](TODO_AND_IMPROVEMENTS.md) ‚Üí "Critical TODOs" section
- Priority P0: End-to-end testing, fix code TODOs, MoE support
- Priority P1: API server, BLAS integration, performance benchmarks
- Priority P2: GPU optimization, production features

### Q: How do I achieve the "distributed-airllm effect"?
**A:** Follow these steps:
1. **Shared Storage**: Ensure all nodes have the same model file (use NFS or copy)
2. **Python Workers**: Use Python workers (not C++ workers) for layer-wise loading
3. **C++ Extensions**: Build cpp_ext for 5-15x speedup
   ```bash
   cd mix/target/airllm/cpp_ext
   python setup.py build_ext --inplace
   ```
4. **Activation Compression**: Enable in config (implemented in Phase 3)
5. **Proper Configuration**: Set appropriate nthreads, buffer-float-type

See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) ‚Üí "Áç≤Âæó Distributed-AirLLM ÊïàÊûúÁöÑÈóúÈçµÈªû"

### Q: What are the limitations?
**A:** See [COMPARISON_AND_ADVANTAGES.md](COMPARISON_AND_ADVANTAGES.md) ‚Üí "Limitations and Trade-offs"
1. **Storage overhead**: Each node needs full model on disk (N √ó model size total)
2. **Node count**: Must be 2^n, max = KV heads in model
3. **Maturity**: 82% complete, needs more testing
4. **Deployment complexity**: More complex than single-machine solutions

### Q: When should I use this?
**A:** **Use Distributed-AirLLM if:**
- ‚úÖ Multiple machines available (2-8 nodes)
- ‚úÖ Limited RAM per node (< 32GB)
- ‚úÖ Need to run large models (30B-70B)
- ‚úÖ Budget constraints (no expensive GPUs)
- ‚úÖ Experimental/research use case

**Don't use if:**
- ‚ùå Single machine with ample RAM (> 128GB) ‚Üí use llama.cpp
- ‚ùå Need maximum throughput ‚Üí use vLLM with GPUs
- ‚ùå Need < 10ms latency ‚Üí use single-GPU solution

## üîß Quick Troubleshooting

### Worker won't connect
```bash
# Check network
ping <root-ip>
nc -zv <root-ip> 9999

# Check firewall
sudo ufw allow 9999/tcp
```

### Out of memory
```bash
# Add more workers to distribute load
# Or increase swap space
sudo fallocate -l 16G /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### Slow performance
```bash
# Build C++ extensions
cd mix/target/airllm/cpp_ext
python setup.py build_ext --inplace

# Verify SIMD enabled
python -c "import tensor_ops_cpp; print(tensor_ops_cpp.get_optimization_info())"
```

### Model file mismatch
```bash
# Verify checksums match
md5sum /path/to/model.m  # Run on all nodes
```

## üìà Performance Expectations

### With C++ Extensions (AVX2 + OpenMP)

| Configuration | Throughput | RAM per Node |
|---------------|------------|--------------|
| 8B, 1 node | 20-30 tok/s | 8-12 GB |
| 8B, 2 nodes | 35-50 tok/s | 4-6 GB each |
| 70B, 4 nodes | 15-25 tok/s | 15-20 GB each |
| 70B, 8 nodes | 25-40 tok/s | 8-12 GB each |

*Actual performance varies by CPU, network, and model*

### Network Traffic (with Phase 3 compression)

- Per token: ~100-200 KB (compressed)
- vs traditional: 73% reduction
- 1Gbps network sufficient for most cases

## üéì Learning Path

### Beginner
1. Read [Quick Start](#-quick-start) above
2. Read [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) ‚Üí "Quick Start" section
3. Try single-machine test deployment

### Intermediate
1. Read [COMPARISON_AND_ADVANTAGES.md](COMPARISON_AND_ADVANTAGES.md)
2. Deploy multi-machine setup
3. Build C++ extensions for speedup

### Advanced
1. Read [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)
2. Read [PHASE3_4_SUMMARY.md](PHASE3_4_SUMMARY.md)
3. Contribute to [TODO_AND_IMPROVEMENTS.md](TODO_AND_IMPROVEMENTS.md)

## ü§ù Contributing

See [TODO_AND_IMPROVEMENTS.md](TODO_AND_IMPROVEMENTS.md) for:
- Current TODOs with priorities
- Areas needing help
- How to contribute

## üìû Support

- **Documentation**: This directory (mix/)
- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions

## üîó External Resources

- **Original Distributed-Llama**: https://github.com/b4rtaz/distributed-llama
- **AirLLM**: https://github.com/lyogavin/airllm

---

**Quick Navigation**:
- [Complete Deployment Guide](DEPLOYMENT_GUIDE.md)
- [Comparison & Advantages](COMPARISON_AND_ADVANTAGES.md)
- [TODO & Improvements](TODO_AND_IMPROVEMENTS.md)
- [Implementation Details](IMPLEMENTATION_SUMMARY.md)
- [Phase 3 & 4 Summary](PHASE3_4_SUMMARY.md)
