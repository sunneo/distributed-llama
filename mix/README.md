# Distributed-AirLLM: Merged Implementation

This directory contains the merged implementation of **Distributed-Llama** and **AirLLM** concepts, creating a system for running large language models (30B+) on distributed consumer hardware.

> ğŸ“– **New to this project?** Start with [QUICK_REFERENCE.md](QUICK_REFERENCE.md) for a quick overview and FAQ.

> ğŸš€ **Want to deploy?** See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for complete deployment instructions.

## Directory Structure

```
mix/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ PLAN.md                     # Detailed task tracking
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md   # Implementation summary
â”œâ”€â”€ src/                        # Reference sources (original implementations)
â”‚   â”œâ”€â”€ airllm/                # Reference: AirLLM concepts
â”‚   â””â”€â”€ distributed-llama.python/  # Reference: Initial implementations
â””â”€â”€ target/                     # Final merged implementation
    â”œâ”€â”€ airllm/                # AirLLM layer-wise inference engine
    â””â”€â”€ distributed-llama.python/  # Python worker for distributed inference
```

## Core Concept: "Shared-Storage Zero-Data Movement"

Instead of distributing model weights across nodes:
- âœ… Every node has the **full model on local SSD**
- âœ… Each node loads **only its assigned layers** into RAM
- âœ… Nodes transmit **only activations** over the network (not weights)
- âœ… Network traffic reduced from GBs to KBs per token

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Root Node (C++)                   â”‚
â”‚  - Orchestrates inference                           â”‚
â”‚  - Loads layers 0-7                                 â”‚
â”‚  - Sends activations to workers                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker 1 (Py)  â”‚  â”‚ Worker 2 (Py)  â”‚
â”‚ Layers 8-15    â”‚  â”‚ Layers 16-23   â”‚
â”‚ Model on SSD   â”‚  â”‚ Model on SSD   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

All nodes: Same 30B model file on local storage
Network: Only activation tensors (~few KB per token)
```

## Components (in target/)

### 1. Distributed-Llama Python Worker (`target/distributed-llama.python/`)

Python implementation of a worker node compatible with the C++ root node.

**Features:**
- âœ… Binary protocol compatibility (socket, ACK, chunked I/O)
- âœ… Config reader (NetConfig, NodeConfig)
- âœ… Activation buffer management (pipes)
- ğŸš§ Tensor operation execution (TODO)
- ğŸš§ Synchronization protocol (TODO)

**Files:**
- `network.py`: TCP socket communication
- `config.py`: Configuration data structures
- `worker.py`: Main worker loop
- `README.md`: Documentation

### 2. AirLLM Layer-wise Engine (`target/airllm/`)

Layer-wise inference engine for memory-efficient model execution.

**Features:**
- âœ… Model header parser (LLAMA, QWEN3, QWEN3_MOE)
- âœ… Weight offset calculator (F32, Q40, Q80)
- âœ… Memory-mapped weight loading (zero-copy)
- âœ… Per-layer weight access
- ğŸš§ Tensor operations (TODO)
- ğŸš§ Layer caching (TODO)

**Files:**
- `model_header.py`: Binary header parser
- `weight_offsets.py`: Byte offset calculator
- `layer_engine.py`: Layer-wise execution engine
- `README.md`: Documentation
- `examples/parse_header.py`: Example usage

## Current Status

### âœ… Completed (Phase 1 & 2 - 55%)

1. **Python Worker Framework**
   - Socket communication with C++ root
   - Configuration synchronization
   - Worker lifecycle management

2. **Model Header Parsing**
   - Binary format parser
   - Support for multiple architectures
   - Quantization format handling

3. **Weight Offset Calculation**
   - Exact byte offsets for all tensors
   - Per-layer and per-weight access
   - Memory-mapped loading support

### ğŸš§ In Progress (Next Steps)

1. **Tensor Operations** (Phase 1.3)
   - RMS normalization
   - Matrix multiplication
   - RoPE (Rotary Position Embedding)
   - Multi-head attention
   - FFN (Feed-forward network)

2. **Activation Synchronization** (Phase 1.4)
   - Receive activations from root
   - Send activations back
   - Handle sync protocol

3. **Layer Caching** (Phase 2.4)
   - LRU cache for hot layers
   - Prefetching strategy
   - Memory pressure management

### ğŸ“‹ Planned (Phases 3-4)

1. **Zero-Data Movement Optimizations** (Phase 3)
   - Shared storage verification
   - Control signal optimization
   - Activation compression

2. **C++ Bottleneck Rewrite** (Phase 4)
   - Profile Python implementation
   - Rewrite hot paths in C++
   - Create pybind11 bindings

## Usage Examples

### Parse Model Header

```bash
cd mix/target/airllm
python examples/parse_header.py /path/to/model.m
```

### Run Python Worker (when complete)

```bash
cd mix/target/distributed-llama.python
python -m worker --host 192.168.1.100 --port 9999 --model /path/to/model.m
```

### Expected Workflow (when complete)

```bash
# Terminal 1: Start root node (C++)
./dllama inference --model model.m --workers 192.168.1.2:9999 192.168.1.3:9999

# Terminal 2: Start Python worker 1
cd mix/target/distributed-llama.python
python -m worker --host 192.168.1.1 --port 9999 --model /mnt/ssd/model.m

# Terminal 3: Start Python worker 2
cd mix/target/distributed-llama.python
python -m worker --host 192.168.1.1 --port 9999 --model /mnt/ssd/model.m
```

## Benefits Over Standard Distributed Inference

| Aspect | Traditional | Distributed-AirLLM |
|--------|------------|---------------------|
| Model Storage | Sharded across nodes | Full model on each node |
| RAM Usage | Full shard in RAM | Only assigned layers |
| Network Traffic | Weights + activations | Activations only |
| Node Addition | Requires rebalancing | Just add worker |
| Fault Tolerance | Lose shard = failure | Any node can load any layer |
| Storage Cost | N Ã— (Model/N) | N Ã— Model |

## Testing

```bash
# Install dependencies
cd mix/target/distributed-llama.python
pip install -r requirements.txt

# Run header parser test (requires actual model file)
cd mix/target/airllm
python examples/parse_header.py /path/to/model.m
```

## Documentation

### ğŸ“š Complete Documentation Suite

- **[éƒ¨ç½²æ‰‹å†Š (Deployment Guide)](DEPLOYMENT_GUIDE.md)** - Complete deployment guide with step-by-step instructions (Chinese/English)
- **[æ¯”è¼ƒå’Œå„ªå‹¢ (Comparison & Advantages)](COMPARISON_AND_ADVANTAGES.md)** - Detailed comparison with AirLLM, Distributed-Llama, and other solutions
- **[TODO å’Œæ”¹é€²å»ºè­° (TODO & Improvements)](TODO_AND_IMPROVEMENTS.md)** - Comprehensive TODO list and improvement suggestions
- **[Implementation Summary](IMPLEMENTATION_SUMMARY.md)** - Technical implementation details
- **[Phase 3 & 4 Summary](PHASE3_4_SUMMARY.md)** - Zero-data movement and C++ optimization details
- **[Development Plan](PLAN.md)** - Detailed task tracking and roadmap

### ğŸš€ Quick Links

- **How to deploy?** â†’ See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)
- **What are the advantages?** â†’ See [COMPARISON_AND_ADVANTAGES.md](COMPARISON_AND_ADVANTAGES.md)
- **What needs improvement?** â†’ See [TODO_AND_IMPROVEMENTS.md](TODO_AND_IMPROVEMENTS.md)

## Development Roadmap

See [`PLAN.md`](PLAN.md) for detailed task tracking.

**Current Status:** 82% complete (Phases 1-4 implemented)

**Next Milestone:** End-to-end testing with C++ root node

## Technical Details

### Binary Protocol

The Python worker implements the same protocol as C++ workers:

```
1. Connect to root
2. Receive ACK
3. Receive NetConfig (batches, nodes, pipes)
4. Send ACK
5. Receive ACK
6. Receive NodeConfig (segments, ops)
7. Send ACK
8. Main loop:
   - Receive sync signal
   - Execute ops
   - Send results
```

### Model File Format

Distributed-llama uses a custom binary format:

```
[Magic: 0x0A00ABCD]
[Header Size: uint32]
[Key-Value Pairs: (key, value) tuples]
[Token Embedding Weights]
[Layer 0 Weights]
[Layer 1 Weights]
...
[Layer N Weights]
[Final Norm]
[Output Classifier]
```

### Weight Offset Calculation

For each layer:
```
offset = header_end + token_emb_size + sum(previous_layer_sizes)

Layer contents:
- Attention norm (dim floats)
- wq, wk, wv, wo (attention weights)
- FFN norm (dim floats)
- w1, w2, w3 (FFN weights)
```

## Contributing

When implementing new features:

1. Work in `target/` directory for final implementation
2. Mark TODOs in code and `PLAN.md`
3. Update progress in `PLAN.md` 
4. Add examples in `target/*/examples/`
5. Document in relevant README

## License

MIT (same as parent project)
